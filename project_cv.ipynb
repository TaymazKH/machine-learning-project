{"cells":[{"cell_type":"markdown","metadata":{"id":"xGegYYVFkfos"},"source":["# Computer Vision Project\n","#### Introduction to Machine learning course | Instructor: Dr. Fatemeh Mirsalehi\n","\n","<font color='cyan'> Responsible TA: Amirhossein Razlighi </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8Hl4dKzkfoy"},"outputs":[],"source":["%pip install torch torchvision numpy matplotlib Pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yc7h8KVHkfo2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"XRtPCwAGkfo4"},"source":["## Section 1: Implementing a CNN from scratch! (30 points)"]},{"cell_type":"markdown","metadata":{"id":"nojbLaXOkfo5"},"source":["### 1.1 - Forward and Backward (20/30 pts)"]},{"cell_type":"markdown","metadata":{"id":"L3xpzuEUkfo6"},"source":["__CNN__ s are powerfull tools to work with in the scope of _Computer Vision_. You have seen MLPs before and know how to do forward and backward pass on them. In this part, we want to create a convolutional layer (a simple one, of course!) just to understand well that how a Convolutional Layer works, behind the scenes. Then, we can simply use PyTorch's `Conv2D` layer for the rest of this project, because we understood that how Convolutions work, behind the scene!"]},{"cell_type":"markdown","metadata":{"id":"ZjqMPe-3kfo6"},"source":["__FORWARD PASS__:\n","\n","In forward pass, you should apply the convolution operation on the input image. The convolution operation is as follows:\n","\n","$$\n","\\text{output}[i, j] = \\sum_{k=0}^{K-1} \\sum_{l=0}^{L-1} \\text{input}[i+k, j+l] \\times \\text{kernel}[k, l]\n","$$\n","\n","You can see a sample convolution operation (with a $3 \\times 3$ kernel) in the following image:\n","<div style=\"text-align:center;\">\n","  <img src=\"./Images/Conv.gif\" />\n","</div>\n","\n","Please note that, these are samples for you to understand the operation better. For _forward pass_ that you should implement, please notice that you should convolve the kernel along all $C$ channels of the input image. So, the output shape should be $(N, H_{out}, W_{out})$. These values are calculated as follows:\n","\n","$$\n","H_{out} = \\frac{H_{in} + 2 \\times \\text{padding}}{\\text{stride} - HH} + 1\n","$$\n","$$\n","W_{out} = \\frac{W_{in} + 2 \\times \\text{padding}}{\\text{stride} - WW} + 1\n","$$\n","\n","\n","where $HH$ and $WW$ are the height and width of the kernel, respectively. __stride__ is the step size of the kernel, and __padding__ is the number of zeros that should be padded to the input image. Please also note that $b$ stands for bias, which is a scalar value that should be added to the output of the convolution operation. (for each kernel)\n","\n","You can see a more detailed example, below:\n","\n","<div style=\"text-align:center;\">\n","  <img src=\"./Images/conv3.gif\" />\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"pYkOPdvakfo7"},"source":["__Backward Pass__:\n","\n","In backward pass, you should calculate gradient of output with respect to the input image and the kernel. These gradients will be used in _optimization_ to update kernel weights. (You saw something similar, in using MLPs with gradient descent algorithm!). So, we need $d_x$, $d_w$ and $d_b$ in the backward pass. These are calculated as follows:\n","\n","$$\n","dx_{padded} [n, :, i * stride: i * stride + HH, j * stride: j * stride + WW] += w[f] \\times dout[n, f, i, j]\n","$$\n","\n","$$\n","dw_{f} = \\sum_{n=0}^{N - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} x_{padded}[n, :, i * stride: i * stride + HH, j * stride: j * stride + WW] \\times dout[n, f, i, j]  \n","$$\n","\n","$$\n","db_{f} = \\sum_{n=0}^{N - 1} \\sum_{i=0}^{H_{out} - 1} \\sum_{j=0}^{W_{out} - 1} dout[n, f, i, j]\n","$$\n","\n","For better understanding of the backward pass, you can see the following image:\n","\n","<div style=\"text-align:center;\">\n","  <img src=\"./Images/backprop_cs231n.png\" />\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqeZ3d3Akfo7"},"outputs":[],"source":["class MyConv:\n","    def __init__(self, stride, padding):\n","        self.stride = stride\n","        self.padding = padding\n","        self.cache = None\n","        self.dx = None\n","        self.dw = None\n","        self.db = None\n","\n","    def forward(self, x, w, b):\n","        N, C, H, W = x.shape\n","        F, _, HH, WW = w.shape\n","        pad = self.padding\n","        stride = self.stride\n","        H_out = (H + 2 * pad - HH) // stride + 1\n","        W_out = (W + 2 * pad - WW) // stride + 1\n","\n","        x_pad = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n","        out = np.zeros((N, F, H_out, W_out))\n","\n","        for n in range(N):\n","            for f in range(F):\n","                for k in range(H_out):\n","                    for l in range(W_out):\n","                        out[n, f, k, l] = np.sum(\n","                            x_pad[n, :, k*stride:k*stride+HH, l*stride:l*stride+WW] * w[f, :, :, :]\n","                        ) + b[f]\n","\n","        self.cache = (x, w, b, x_pad)\n","        return out\n","\n","    def backward(self, dout):\n","        x, w, b, x_padded = self.cache\n","\n","        N, C, H, W = x.shape\n","        F, _, HH, WW = w.shape\n","        _, _, H_OUT, W_OUT = dout.shape\n","\n","        dw = np.zeros_like(w)\n","        dx = np.zeros_like(x)\n","        db = np.zeros_like(b)\n","        dx_padded = np.zeros_like(x_padded)\n","\n","        for n in range(N):\n","            for f in range(F):\n","                for i in range(H_OUT):\n","                    for j in range(W_OUT):\n","                        dw[f] += x_padded[n, :, i*self.stride:i*self.stride+HH, j*self.stride:j*self.stride+WW] * dout[n, f, i, j]\n","                        dx_padded[n, :, i*self.stride:i*self.stride+HH, j*self.stride:j*self.stride+WW] += w[f] * dout[n, f, i, j]\n","                db[f] += np.sum(dout[n, f, :, :])\n","\n","        dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding] if self.padding > 0 else dx_padded\n","\n","        self.dx = dx\n","        self.dw = dw\n","        self.db = db\n","        return dx, dw, db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uq9ZIZ7Gkfo8"},"outputs":[],"source":["def rel_error(x, y):\n","  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dErRar_kfo8"},"outputs":[],"source":["# A simple test for forward pass (DO NOT CHANGE)\n","\n","x_shape = (2, 3, 4, 4)\n","w_shape = (3, 3, 4, 4)\n","x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n","w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n","b = np.linspace(-0.1, 0.2, num=3)\n","\n","out = MyConv(stride=2, padding=1).forward(x, w, b)\n","correct_out = np.array([[[[[-0.08759809, -0.10987781],\n","                           [-0.18387192, -0.2109216 ]],\n","                          [[ 0.21027089,  0.21661097],\n","                           [ 0.22847626,  0.23004637]],\n","                          [[ 0.50813986,  0.54309974],\n","                           [ 0.64082444,  0.67101435]]],\n","                         [[[-0.98053589, -1.03143541],\n","                           [-1.19128892, -1.24695841]],\n","                          [[ 0.69108355,  0.66880383],\n","                           [ 0.59480972,  0.56776003]],\n","                          [[ 2.36270298,  2.36904306],\n","                           [ 2.38090835,  2.38247847]]]]])\n","\n","# The outputted difference which is printed, should be around 1e-8\n","print ('Testing conv_forward_naive')\n","print ('difference: ', rel_error(out, correct_out))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlD_yv62kfo8"},"outputs":[],"source":["def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","  grad = np.zeros_like(x)\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","    ix = it.multi_index\n","\n","    oldval = x[ix]\n","    x[ix] = oldval + h\n","    pos = f(x).copy()\n","    x[ix] = oldval - h\n","    neg = f(x).copy()\n","    x[ix] = oldval\n","\n","    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","    it.iternext()\n","  return grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVei9-5jkfo9"},"outputs":[],"source":["# A simple test for backward pass (DO NOT CHANGE)\n","\n","x = np.random.randn(4, 3, 5, 5)\n","w = np.random.randn(2, 3, 3, 3)\n","b = np.random.randn(\n","    2,\n",")\n","dout = np.random.randn(4, 2, 5, 5)\n","conv = MyConv(stride=1, padding=1)\n","\n","dx_num = eval_numerical_gradient_array(\n","    lambda x: conv.forward(x, w, b), x, dout\n",")\n","dw_num = eval_numerical_gradient_array(\n","    lambda w: conv.forward(x, w, b), w, dout\n",")\n","db_num = eval_numerical_gradient_array(\n","    lambda b: conv.forward(x, w, b), b, dout\n",")\n","\n","out = conv.forward(x, w, b)\n","dx, dw, db = conv.backward(dout)\n","\n","# Your printed errors should be around 1e-9\n","print(\"Testing conv_backward_naive function\")\n","print(\"dx error: \", rel_error(dx, dx_num))\n","print(\"dw error: \", rel_error(dw, dw_num))\n","print(\"db error: \", rel_error(db, db_num))"]},{"cell_type":"markdown","metadata":{"id":"ASnzPa8ikfo9"},"source":["### 1.2 - Visualizing Convolution results (10/30 pts)"]},{"cell_type":"markdown","metadata":{"id":"PylzpG9Mkfo9"},"source":["In this part, we try to visualize the results of the convolution operation. We will 2 sample photos (in `Images/Sample`) and then we will use our `MyConv` class to apply some cool convolutions (and see their result on an image)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DZvK1Ufkfo9"},"outputs":[],"source":["from PIL import Image\n","\n","first_img, second_img = Image.open(\"./Images/Sample/Sharif_1.JPG\"), Image.open(\n","    \"./Images/Sample/Sharif_2.jpg\"\n",")\n","first_img = first_img.resize((256, 256))\n","second_img = second_img.resize((256, 256))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL82qHJ9kfo9"},"outputs":[],"source":["first_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WH5AdCvkfo-"},"outputs":[],"source":["second_img"]},{"cell_type":"markdown","metadata":{"id":"X1GljeJPkfo-"},"source":["Convolutions have some interesting usages in image processing. For example, you can extract edges from an image by applying a convolution with a kernel that detects edges. Or you can convolve a specific kernel with your image to make it grayscale or blurred! Look at the definitions below:\n","\n","__Edge Detection Kernel__:\n","\n","_Sobel_ is one of the most famous edge detection kernels. It has two kernels, one for detecting vertical edges and the other for horizontal edges. You can see the kernels below:\n","$$\n","\\text{Sobel}_x = \\begin{bmatrix}\n","-1 & 0 & 1 \\\\\n","-2 & 0 & 2 \\\\\n","-1 & 0 & 1\n","\\end{bmatrix}\n","$$\n","\n","$$\n","\\text{Sobel}_y = \\begin{bmatrix}\n","-1 & -2 & -1 \\\\\n","0 & 0 & 0 \\\\\n","1 & 2 & 1\n","\\end{bmatrix}\n","$$\n","\n","\n","__Grayscale Kernel__:\n","\n","You can simply convolve the image with the following kernel to make it grayscale:\n","\n","$$\n","\\text{Grayscale\\_across\\_R} = \\begin{bmatrix}\n","0 & 0 & 0 \\\\\n","0 & 0.3 & 0 \\\\\n","0 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","$$\n","\\text{Grayscale\\_across\\_G} = \\begin{bmatrix}\n","0 & 0 & 0 \\\\\n","0 & 0.6 & 0 \\\\\n","0 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","$$\n","\\text{Grayscale\\_across\\_B} = \\begin{bmatrix}\n","0 & 0 & 0 \\\\\n","0 & 0.1 & 0 \\\\\n","0 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","(Convolve each kernel with its corresponding channel)\n","\n","__Blurring Kernel__:\n","\n","One of the famous kernels to blur an image is _Gaussian Blur_. You can see the kernel below:\n","\n","$$\n","\\text{Gaussian\\_Blur} = \\frac{1}{16} \\times \\begin{bmatrix}\n","1 & 2 & 1 \\\\\n","2 & 4 & 2 \\\\\n","1 & 2 & 1\n","\\end{bmatrix}\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQbEeGv_kfo_"},"outputs":[],"source":["img_size = 256\n","\n","x = np.zeros((2, 3, img_size, img_size))\n","x[0] = np.array(first_img).transpose(2, 0, 1)\n","x[1] = np.array(second_img).transpose(2, 0, 1)\n","\n","# Defining convolution kernels\n","sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n","sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n","grayscale_R = np.array([[0, 0, 0], [0, 0.3, 0], [0, 0, 0]])\n","grayscale_G = np.array([[0, 0, 0], [0, 0.6, 0], [0, 0, 0]])\n","grayscale_B = np.array([[0, 0, 0], [0, 0.1, 0], [0, 0, 0]])\n","gaussian_blur = (1/16) * np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]])\n","\n","# A convolution weight, holding 4 filters 3x3\n","w = np.stack([\n","    np.stack([sobel_x, sobel_x, sobel_x]),\n","    np.stack([sobel_y, sobel_y, sobel_y]),\n","    np.stack([grayscale_R, grayscale_G, grayscale_B]),\n","    np.stack([gaussian_blur, gaussian_blur, gaussian_blur])\n","])\n","\n","b = np.array([0, 128, 128, 0])  # todo: chatgpt says [0,0,0,0]. what to do?\n","\n","conv = MyConv(stride=1, padding=1)\n","out = conv.forward(x, w, b)"]},{"cell_type":"code","source":["titles = ['Sobel X', 'Sobel Y', 'Grayscale', 'Gaussian Blur']\n","\n","plt.figure(figsize=(12, 6))\n","plt.subplot(2, 5, 1)\n","plt.imshow(first_img)\n","plt.title('1st Image')\n","plt.axis('off')\n","for i in range(4):\n","    plt.subplot(2, 5, i + 2)\n","    plt.imshow(out[0, i, :, :], cmap='gray')\n","    plt.title(titles[i])\n","    plt.axis('off')\n","plt.show()\n","\n","plt.figure(figsize=(12, 6))\n","plt.subplot(2, 5, 1)\n","plt.imshow(second_img)\n","plt.title('2nd Image')\n","plt.axis('off')\n","for i in range(4):\n","    plt.subplot(2, 5, i + 2)\n","    plt.imshow(out[1, i, :, :], cmap='gray')\n","    plt.title(titles[i])\n","    plt.axis('off')\n","plt.show()"],"metadata":{"id":"hJcR3ZR5na1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrICQWqikfo_"},"source":["Now that we understood how Convolution works, we proceed further to use it in our models to perform a desired and specific task! We can use PyTorch's `Conv2D` layer from now on, because we understood how Convolutions work, behind the scene!"]},{"cell_type":"markdown","metadata":{"id":"JNyjTCk2kfo_"},"source":["## Section 2: Neural Style Transfer (90 points)"]},{"cell_type":"markdown","metadata":{"id":"RavXCeEbkfpA"},"source":["In this part, we are going to implement a famous technique in the field of _Computer Vision_ called __Neural Style Transfer__. This technique is used to apply the style of a specific image to another image. For example, you can apply the style of a famous painting to your photo! Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"rONw0a1pkfpA"},"source":["### 2.1 - Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbDRlsdXkfpA"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import torchvision.transforms as transforms\n","from torchvision.models import vgg19, VGG19_Weights\n","\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCmsGsPOkfpA"},"outputs":[],"source":["device = torch.device(\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",")\n","torch.set_default_device(device)\n","device"]},{"cell_type":"markdown","metadata":{"id":"xBsvuYbKkfpA"},"source":["### 2.2 - Loading Images and doing some Preprocessing (5/90 pts)"]},{"cell_type":"markdown","metadata":{"id":"Ad4WOrcIkfpA"},"source":["In this part, you should load the content and style images and preprocess them. Preprocessing includes resizing the images to a specific size, normalizing them, and converting them to PyTorch tensors. You can use `torchvision.transforms` to do these tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjdiKmdkkfpB"},"outputs":[],"source":["img_size = 512 if not device == \"cpu\" else 256\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSJNLa0skfpB"},"outputs":[],"source":["def load_image(image_path, transform):\n","    return transform(Image.open(image_path).convert('RGB')).unsqueeze(0).to(device)\n","\n","style_image = load_image('./Images/Neural_Transfer/style.jpeg', transform)\n","content_image = load_image('./Images/Neural_Transfer/Content.jpeg', transform)\n","\n","style_image.size(), content_image.size()"]},{"cell_type":"markdown","metadata":{"id":"zMC6FXeZkfpB"},"source":["Here, you should implement a helper function to show you images (which are now in form of pytorch tensors) in the notebook. You can use `torchvision.transforms.ToPILImage` to convert a tensor to a PIL image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAd2UuFokfpC"},"outputs":[],"source":["def imshow(tensor, title=None):\n","    image = tensor.cpu().clone()\n","    image = image.squeeze(0)\n","    image = transforms.ToPILImage()(image)\n","    plt.imshow(image)\n","    if title:\n","        plt.title(title)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZkIqQ3hkfpC"},"outputs":[],"source":["imshow(style_image, \"Style Image\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMNKLxkNkfpC"},"outputs":[],"source":["imshow(content_image, \"Content Image\")"]},{"cell_type":"markdown","metadata":{"id":"F1grJjhJkfpC"},"source":["### 2.2 - VGG19 Model (15/90 pts)"]},{"cell_type":"markdown","metadata":{"id":"kc-nJsrukfpC"},"source":["One of the important parts of Neural Style Transfer is to use a pre-trained model to extract features from the content and style images. Here, we are going to use a pre-trained VGG19 model to extract features from the images. You can use `torchvision.models.vgg19` to load the model. You should also freeze the weights of the model to prevent them from updating during the optimization process."]},{"cell_type":"markdown","metadata":{"id":"YP9xHK_0kfpC"},"source":["<div style=\"text-align:center;\">\n","  <img src=\"./Images/VGG-19.png\" />\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"0TEQ-5zFkfpC"},"source":["But what is VGG model? Let's see with a simple experiment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsjeDdt5kfpD"},"outputs":[],"source":["import certifi\n","import os\n","\n","os.environ['SSL_CERT_FILE'] = certifi.where()"]},{"cell_type":"markdown","metadata":{"id":"JUnhiiqYkfpD"},"source":["Here, please load `VGG19` model from `torchvision.models` and see the architecture of the model. You may need models `features` and `classifier` part stored seperately as we will discuss these both parts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3oLbzpMkfpD"},"outputs":[],"source":["vgg_model = vgg19(weights=VGG19_Weights.DEFAULT).to(device)\n","vgg_model_features = vgg_model.features\n","vgg_model_classifier = vgg_model.classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Xoutf1vVkfpD"},"outputs":[],"source":["import torchsummary\n","torchsummary.summary(vgg_model, input_size=(3, img_size, img_size))"]},{"cell_type":"markdown","metadata":{"id":"UZ7L-D9NkfpD"},"source":["Please briefly explain what is a VGG model used for and what does each `features` and `classifier` part, represent?"]},{"cell_type":"markdown","metadata":{"id":"qOhYMbZakfpE"},"source":["**Your Answer:**\n","\n","The VGG (Visual Geometry Group) model is a convolutional neural network architecture. It is widely used in computer vision tasks such as image classification, feature extraction, object detection, and image segmentation.\n","\n","- Features: This part consists of a series of convolutional layers followed by max-pooling layers. The primary role of the features part is to extract spatial hierarchies of features from the input images.\n","- Classifier: This part consists of fully connected layers (aka dense layers). This part's role is to take the high-level features extracted by the convolutional layers and perform the final classification."]},{"cell_type":"markdown","metadata":{"id":"Czt7J0BMkfpE"},"source":["Now, we are going to give a sample image `Images/Neural_Transfer/Car.jpeg` to the model, see the output for classification and also visualize the feature maps (outputs of each convolutional layer) to see what's going on!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLWsmiEBkfpE"},"outputs":[],"source":["vgg_test_image = Image.open(\"./Images/Neural_Transfer/Car.jpeg\")\n","plt.imshow(vgg_test_image)\n","plt.title('Test Image')\n","plt.axis('off')\n","plt.show()\n","\n","\n","preprocess_vgg = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","input_tensor = preprocess_vgg(vgg_test_image).unsqueeze(0).to(device)\n","vgg_model.eval()\n","with torch.no_grad():\n","    output = vgg_model(input_tensor)\n","_, predicted = torch.max(output, 1)\n","print(f'Predicted class for test image: {predicted}')"]},{"cell_type":"markdown","metadata":{"id":"5Zeh95qckfpE"},"source":["As you can see, the output of the classifier is an integer. This integer is actually the **class number** in _Image Net_ dataset. Let's get the name of the class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8tgQhtYkfpE"},"outputs":[],"source":["import requests\n","\n","class_idx = requests.get(\n","    \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",").json()\n","\n","# Get the class name\n","class_name = class_idx[predicted]\n","\n","print(f\"Predicted class: {class_name}\")"]},{"cell_type":"markdown","metadata":{"id":"dgRnxNSpkfpF"},"source":["Now that we understood the architecture of VGG, and also the output of its classifier, it's time to extract features from the images! And see what's going on in the feature maps!"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"627zaEKKkfpF"},"outputs":[],"source":["def get_feature_maps(model, x):\n","    feature_maps = []\n","    for layer in model:\n","        x = layer(x)\n","        if isinstance(layer, nn.Conv2d):\n","            feature_maps.append(x)\n","    return feature_maps\n","\n","def visualize_feature_maps(feature_maps):\n","    for i, fm in enumerate(feature_maps):\n","        fm = fm.squeeze(0)\n","        plt.figure(figsize=(10, 10))\n","        for j in range(min(16, fm.shape[0])):\n","            ax = plt.subplot(4, 4, j + 1)\n","            plt.imshow(fm[j].cpu().detach().numpy(), cmap='coolwarm')\n","            ax.axis('off')\n","        plt.suptitle(f\"Feature Maps - Layer {i + 1}\")\n","        plt.show()\n","\n","feature_maps = get_feature_maps(vgg_model_features, input_tensor)\n","visualize_feature_maps(feature_maps)"]},{"cell_type":"markdown","metadata":{"id":"AL3KIYyGkfpF"},"source":["It's fun too look under the hood, isn't it? As you can see, we can guess what are the learned features in each layer if the network (at least in the first layers). For example, in the first layer, the network learns to detect edges, and in the second layer, it learns to detect simple shapes. In the deeper layers, the network learns to detect more complex features."]},{"cell_type":"markdown","metadata":{"id":"cC6ZJI1DkfpF"},"source":["### 2.3 - Content and Style Loss (15/90 pts)"]},{"cell_type":"markdown","metadata":{"id":"3tUN0e6MkfpG"},"source":["In order to learn the correct image and keep the content of the original image while applying the style of the seconf image, we need 2 losses: 1. Content Loss and 2. Style Loss. Let's delve into each one!"]},{"cell_type":"markdown","metadata":{"id":"mHP2QGKEkfpG"},"source":["**Content Loss** is responsible for keeping the content of the original image. It is calculated as the mean squared error between the feature maps of the original image and the generated image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLqafjTHkfpG"},"outputs":[],"source":["class ContentLoss(nn.Module):\n","    def __init__(self, target):\n","        super(ContentLoss, self).__init__()\n","        self.target = target.detach()\n","        self.loss = None\n","\n","    def forward(self, input_f):\n","        self.loss = nn.functional.mse_loss(input_f, self.target)\n","        return input_f"]},{"cell_type":"markdown","metadata":{"id":"06AUQxwPkfpG"},"source":["**Style Loss** is responsible for applying the style of the second image to the generated image. It is calculated as the mean squared error between the Gram matrices of the feature maps of the second image and the generated image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vXhGDOckfpG"},"outputs":[],"source":["def gram_calculator(input_f):\n","    \"\"\"\n","    Calculate the normalized Gram Matrix of a given input\n","    \"\"\"\n","    a, b, c, d = input_f.size()\n","    features = input_f.view(a * b, c * d)\n","    G = torch.mm(features, features.t())\n","    return G.div(a * b * c * d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsVrvLWNkfpH"},"outputs":[],"source":["class StyleLoss(nn.Module):\n","    def __init__(self, target_feature):\n","        super(StyleLoss, self).__init__()\n","        self.target = gram_calculator(target_feature).detach()\n","        self.loss = None\n","\n","    def forward(self, input_f):\n","        self.loss = nn.functional.mse_loss(gram_calculator(input_f), self.target)\n","        return input_f"]},{"cell_type":"markdown","metadata":{"id":"iwBDPVKkkfpH"},"source":["Now, we use `features` part of the VGG model to extract features from the content and style images. Then, we calculate the content and style loss using the extracted features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K98AcQ6nkfpH"},"outputs":[],"source":["cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.to(device).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"513zED-UkfpH"},"outputs":[],"source":["cnn_normalization_mean = torch.tensor(\n","    [0.485, 0.456, 0.406]\n",").to(device)  # Normalization mean for the VGG19 model\n","cnn_normalization_std = torch.tensor(\n","    [0.229, 0.224, 0.225]\n",").to(device)  # Normalization std for the VGG19 model\n","\n","# Normalize the images in order to feed it to the VGG19 model\n","transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=cnn_normalization_mean, std=cnn_normalization_std)\n","])\n","\n","content_image = load_image('./Images/Neural_Transfer/Content.jpeg', transform)\n","style_image = load_image('./Images/Neural_Transfer/style.jpeg', transform)"]},{"cell_type":"markdown","metadata":{"id":"92zkXD9tkfpI"},"source":["Please note that after which layers you should calculate the content loss and after which ones, you should calculate the style loss. Use the layer names you saw earlier in the `torchsummary` output for VGG19 model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsyJl6BFkfpI"},"outputs":[],"source":["content_layers_default = ['conv_21']\n","style_layers_default = ['conv_1', 'conv_6', 'conv_11', 'conv_20', 'conv_29']\n","\n","def get_features(image, model, layers):\n","    features = {}\n","    x = image\n","    for name, layer in model._modules.items():\n","        x = layer(x)\n","        if name in layers:\n","            features[layers[name]] = x\n","    return features\n","\n","layer_map = {\n","    '0': 'conv_1',\n","    '5': 'conv_6',\n","    '10': 'conv_11',\n","    '19': 'conv_20',\n","    '21': 'conv_21',\n","    '28': 'conv_29'\n","}\n","\n","content_features = get_features(content_image, cnn, layer_map)\n","style_features = get_features(style_image, cnn, layer_map)\n","\n","content_loss_layers = [ContentLoss(content_features[layer]) for layer in content_layers_default]\n","style_loss_layers = [StyleLoss(style_features[layer]) for layer in style_layers_default]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Jjij3l7kfpJ"},"outputs":[],"source":["model = nn.Sequential()\n","i = 0\n","content_losses = []\n","style_losses = []\n","\n","for layer in cnn.children():\n","    if isinstance(layer, nn.Conv2d):\n","        i += 1\n","        name = f\"conv_{i}\"\n","    elif isinstance(layer, nn.ReLU):\n","        name = f\"relu_{i}\"\n","        layer = nn.ReLU(inplace=False)\n","    elif isinstance(layer, nn.MaxPool2d):\n","        name = f\"pool_{i}\"\n","    elif isinstance(layer, nn.BatchNorm2d):\n","        name = f\"bn_{i}\"\n","    else:\n","        raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n","    model.add_module(name, layer)\n","    if name in content_layers_default:\n","        target = model(content_image).detach()\n","        content_loss = ContentLoss(target)\n","        model.add_module(f\"content_loss_{i}\", content_loss)\n","        content_losses.append(content_loss)\n","    if name in style_layers_default:\n","        target_feature = model(style_image).detach()\n","        style_loss = StyleLoss(target_feature)\n","        model.add_module(f\"style_loss_{i}\", style_loss)\n","        style_losses.append(style_loss)\n","\n","for i in range(len(model) - 1, -1, -1):\n","    if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n","        break\n","\n","model = model[:i+1]\n","\n","plt.imshow(Image.open('./Images/Neural_Transfer/Content.jpeg').convert('RGB'))\n","plt.title('Input Image')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lQNFoE18kfpK"},"source":["(15/90 pts) Now, we need to define our optimizer. We use `LBFGS` optimizer. Let's get familiar with this optimizer a little bit:"]},{"cell_type":"markdown","metadata":{"id":"_PQPVb-hkfpL"},"source":["Let's use this optimizer to optimize a simple function $f(x) = x^2$. We want to find the minimum of this function using `LBFGS` optimizer. First, you should initialize a tensor $x$ with a random value. Then, you should define a closure function that calculates the value of the function and its gradient. Finally, you should use the `step` method of the optimizer to update the value of $x$.\n","\n","And lastly, you should generate an animation using `matplotlib.animation` to show how the value of $x$ changes during optimization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnmVZ9pTkfpL"},"outputs":[],"source":["import matplotlib.animation as animation\n","from torch.optim import LBFGS\n","import IPython.display as display\n","from random import uniform\n","\n","\n","def quadratic_function(x):\n","    return x**2\n","\n","x = torch.tensor([uniform(-10, 10)], requires_grad=True)\n","optimizer = LBFGS([x])\n","\n","\n","# Closure function and optimization loop\n","x_values = []\n","y_values = []\n","\n","def closure():\n","    optimizer.zero_grad()\n","    y = quadratic_function(x)\n","    y.backward()\n","    x_values.append(x.item())\n","    y_values.append(y.item())\n","    return y\n","\n","for _ in range(20):\n","    optimizer.step(closure)\n","\n","\n","# Animation\n","fig, ax = plt.subplots()\n","ax.set_xlim(-11, 11)\n","ax.set_ylim(0, 101)\n","ax.set_xlabel('x')\n","ax.set_ylabel('f(x)')\n","ax.set_title('Optimization Process of f(x)')\n","line, = ax.plot([], [], lw=2)\n","point, = ax.plot([], [], 'ro')\n","\n","def init():\n","    line.set_data([], [])\n","    point.set_data([], [])\n","    return line, point\n","\n","def animate(i):\n","    x_vals = x_values[:i+1]\n","    y_vals = y_values[:i+1]\n","    line.set_data(x_vals, y_vals)\n","    point.set_data(x_vals[-1], y_vals[-1])\n","    return line, point\n","\n","plot_animation = animation.FuncAnimation(fig, animate, init_func=init, frames=len(x_values), interval=200, blit=True)\n","plt.close()\n","display.HTML(plot_animation.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"yKGYXjn1kfpM"},"source":["(35/90 pts) Now, let's use this optimizer to optimize our `input_image` to minimize the content and style loss. You should define a closure function that calculates the content and style loss and their gradients. Then, you should use the `step` method of the optimizer to update the value of the `input_image`. Please also **note** that you should use a weighted sum of the content and style loss to optimize the input image. (weight of each loss is a hyperparameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84dpfBYqkfpN"},"outputs":[],"source":["# Train your model using LBFGS optimizer and 2 implemented losses (style loss and content loss)\n","# You should report the `style loss`, `content loss`, and `total loss` for each iteration. Note that `total loss`\n","# is the sum of `style loss` and `content loss` with some weights (alpha and beta) which you should tune them.\n","# Also, please plot the image each 50 iterations.\n","# An important note is that your optimizer might change the values of the image tensor in a way that the pixel values\n","# are no longer in the range of [0, 1] (in normalized format). So, you should clip the values of the image tensor\n","# to be in the range of [0, 1] after each iteration, to prevent any problems occuring to your image.\n","\n","\n","def get_style_model_and_losses(\n","    cnn,\n","    content_image,\n","    style_image,\n","    content_layers=content_layers_default,\n","    style_layers=style_layers_default\n","):\n","    content_losses = []\n","    style_losses = []\n","    model = nn.Sequential()\n","    i = 0\n","    for layer in cnn.children():\n","        if isinstance(layer, nn.Conv2d):\n","            i += 1\n","            name = f'conv_{i}'\n","        elif isinstance(layer, nn.ReLU):\n","            name = f'relu_{i}'\n","            layer = nn.ReLU(inplace=False)\n","        elif isinstance(layer, nn.MaxPool2d):\n","            name = f'pool_{i}'\n","        elif isinstance(layer, nn.BatchNorm2d):\n","            name = f'bn_{i}'\n","        else:\n","            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n","        model.add_module(name, layer)\n","        if name in content_layers:\n","            target = model(content_image).detach()\n","            content_loss = ContentLoss(target)\n","            model.add_module(f\"content_loss_{i}\", content_loss)\n","            content_losses.append(content_loss)\n","        if name in style_layers:\n","            target_feature = model(style_image).detach()\n","            style_loss = StyleLoss(target_feature)\n","            model.add_module(f\"style_loss_{i}\", style_loss)\n","            style_losses.append(style_loss)\n","    for i in range(len(model) - 1, -1, -1):\n","        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n","            break\n","    model = model[:i + 1]\n","    return model, content_losses, style_losses\n","\n","\n","content_image = load_image('./Images/Neural_Transfer/Content.jpeg', transform)\n","style_image = load_image('./Images/Neural_Transfer/style.jpeg', transform)\n","model, content_losses, style_losses = get_style_model_and_losses(cnn, content_image, style_image)\n","content_losses_list = []\n","style_losses_list = []\n","total_losses_list = []\n","\n","content_image = load_image('./Images/Neural_Transfer/Content.jpeg', transform)\n","style_image = load_image('./Images/Neural_Transfer/style.jpeg', transform)\n","input_img = content_image.clone().requires_grad_(True)\n","optimizer = optim.LBFGS([input_img])\n","\n","\n","# Training loop\n","num_steps = 151\n","alpha = 10**5\n","beta = 10**1\n","\n","for i in range(num_steps):\n","    def closure():\n","        input_img.data.clamp_(0, 1)\n","        optimizer.zero_grad()\n","        model(input_img)\n","\n","        content_score = sum([l.loss for l in content_losses])\n","        style_score = sum([l.loss for l in style_losses])\n","        content_score *= alpha\n","        style_score *= beta\n","        total_loss = style_score + content_score\n","        # total_loss = content_score * alpha + style_score * beta\n","        total_loss.backward()\n","        content_losses_list.append(content_score)\n","        style_losses_list.append(style_score.item())\n","        total_losses_list.append(total_loss.item())\n","\n","        if i % 50 == 0:\n","            print(f'Iteration {i}:')\n","            print(f'Style Loss: {style_score.item()}')\n","            print(f'Content Loss: {content_score}')\n","            print(f'Total Loss: {total_loss.item()}')\n","            imshow(input_img, f'Step {i}')\n","        return total_loss\n","\n","    optimizer.step(closure)"]},{"cell_type":"markdown","metadata":{"id":"Nq8wjGlRkfpP"},"source":["Now, let's see the result of optimization process on our `input_image`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Fst8fhHkfpQ"},"outputs":[],"source":["  input_img.data.clamp_(0, 1)\n","imshow(input_img, title='Final Image')"]},{"cell_type":"markdown","metadata":{"id":"FMOg_s8gkfpQ"},"source":["(5/90 pts) Also, let's plot our `style_losses`, `content_losses` and `total_losses` to see how they change during the optimization process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gADjthsekfpR"},"outputs":[],"source":["#TODO: Plot the total weighted loss w.r.t. the iteration number\n","plt.figure(figsize=(10, 5))\n","plt.plot(total_losses_list)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Total Loss during optimization')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvEPYjlxkfpS"},"outputs":[],"source":["# TODO: Plot the style loss w.r.t. the iteration number\n","plt.figure(figsize=(10, 5))\n","plt.plot(style_losses_list)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Style Loss during optimization')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umsGJp8DkfpT"},"outputs":[],"source":["# TODO: Plot the content loss w.r.t. the iteration number\n","plt.figure(figsize=(10, 5))\n","plt.plot(content_losses_list)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Content Loss during optimization')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDzvlD4ekfpT"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}